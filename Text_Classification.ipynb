{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/KsvJA7aqKiuupD5OnrFa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANSHULAGARWAL09/Text-Classification-for-3.6M-Amazon-Reviews-using-LSA/blob/master/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynVfifjGa2Yu",
        "colab_type": "text"
      },
      "source": [
        "**Text Classification for 3.6M Amazon Reviews**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lZbFWLRavEl",
        "colab_type": "text"
      },
      "source": [
        "The goal of text classification is to automatically classify the text documents into one or more defined categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5v5M19yaywr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32Of239mBiEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "384a2996-3e5d-4994-9e55-2ad52f2166fc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK2lj1RTdI8L",
        "colab_type": "text"
      },
      "source": [
        "***1. DATASET PREPARATION***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FYCsXq_cRKz",
        "colab_type": "text"
      },
      "source": [
        "The dataset consists of 3.6M text reviews and their labels, we will use only a small fraction of data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64lYlp2Rbdl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the dataset\n",
        "data = open('corpus').read()\n",
        "labels, texts = [], []\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "    content = line.split()\n",
        "    labels.append(content[0])\n",
        "    texts.append(\" \".join(content[1:]))\n",
        "\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = pandas.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcszf2LbcsYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSsS8gDydV4z",
        "colab_type": "text"
      },
      "source": [
        "**2. FEATURE ENGINEERING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrZLn6Wcdh99",
        "colab_type": "text"
      },
      "source": [
        "2.1 Count Vectors as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuQvYnYoc01f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['text'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Ij1mVYek-0",
        "colab_type": "text"
      },
      "source": [
        "2.2 TF-IDF Vectors as features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEpDQsGLer-r",
        "colab_type": "text"
      },
      "source": [
        "a. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef2NlMClehYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(trainDF['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9yQssm2e-tC",
        "colab_type": "text"
      },
      "source": [
        "b. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iJlxTdme7dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk1bF2u-fHTg",
        "colab_type": "text"
      },
      "source": [
        "c. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3XvHfjKfCTp",
        "colab_type": "code",
        "outputId": "fd5ab70c-c918-4082-f6f5-94023d9d2002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdKtWdNPfStd",
        "colab_type": "text"
      },
      "source": [
        "2.3 Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rpfthjYfcLM",
        "colab_type": "text"
      },
      "source": [
        "Word embeddings can be trained using the input corpus itself or can be generated using pre-trained word embeddings such as Glove, FastText, and Word2Vec. Any one of them can be downloaded and used as transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnJcKQXffi0I",
        "colab_type": "text"
      },
      "source": [
        "Following snippet shows how to use pre-trained word embeddings in the model. There are four essential steps:\n",
        "\n",
        "-> Loading the pretrained word embeddings\n",
        "-> Creating a tokenizer object\n",
        "-> Transforming text documents to sequence of tokens and pad them\n",
        "-> Create a mapping of token and their respective embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exTF6xQbfLOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the pre-trained word-embedding vectors \n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open(r'wiki-news-300d-1M.vec','w+b')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOpEHpboiDN3",
        "colab_type": "text"
      },
      "source": [
        "2.4 Text / NLP based features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vxK7OfyiOH8",
        "colab_type": "text"
      },
      "source": [
        "A number of extra text based features can also be created which sometimes are helpful for improving text classification models. Some examples are:\n",
        "\n",
        "-> Word Count of the documents – total number of words in the documents\n",
        "-> Character Count of the documents – total number of characters in the documents\n",
        "-> Average Word Density of the documents – average length of the words used in the documents\n",
        "-> Punctuation Count in the Complete Essay – total number of punctuation marks in the documents\n",
        "-> Upper Case Count in the Complete Essay – total number of upper count words in the documents\n",
        "-> Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n",
        "-> Frequency distribution of Part of Speech Tags:\n",
        ".Noun Count\n",
        ".Verb Count\n",
        ".Adjective Count\n",
        ".Adverb Count\n",
        ".Pronoun Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDLwF-cyiEFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainDF['char_count'] = trainDF['text'].apply(len)\n",
        "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
        "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
        "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "\n",
        "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsjp3Lxziwbd",
        "colab_type": "text"
      },
      "source": [
        "2.5 Topic Models as features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx3MOKgxi5JB",
        "colab_type": "text"
      },
      "source": [
        "Topic Modelling is a technique to identify the groups of words (called a topic) from a collection of documents that contains bestinformation in the collection. I have used Latent Dirichlet Allocation for generating Topic Modelling Features. LDA is an iterative model which starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrys_GCkixOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(xtrain_count)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = count_vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoIj8xQcjFCy",
        "colab_type": "text"
      },
      "source": [
        "**3. MODEL BUILDING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl1yGPQ4jUrP",
        "colab_type": "text"
      },
      "source": [
        "1.Naive Bayes Classifier\n",
        "2.Linear Classifier\n",
        "3.Support Vector Machine\n",
        "4.Bagging Models\n",
        "5.Boosting Models\n",
        "6.Shallow Neural Networks\n",
        "7.Deep Neural Networks:-\n",
        "(i) Convolutional Neural Network (CNN)\n",
        "(ii) Long Short Term Modelr (LSTM)\n",
        "(iii) Gated Recurrent Unit (GRU)\n",
        "(iv) Bidirectional RNN\n",
        "(v) Recurrent Convolutional Neural Network (RCNN)\n",
        "(vi) Other Variants of Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiRJcAgOjzgS",
        "colab_type": "text"
      },
      "source": [
        "The following function is a utility function which can be used to train a model. It accepts the classifier, feature_vector of training data, labels of training data and feature vectors of valid data as inputs. Using these inputs, the model is trained and accuracy score is computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9LFdDMRjEir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):    \n",
        "    # fit the training dataset on the classifier    \n",
        "    classifier.fit(feature_vector_train, label)        \n",
        "    # predict the labels on validation dataset    \n",
        "    predictions = classifier.predict(feature_vector_valid)        \n",
        "    if is_neural_net:        \n",
        "        predictions = predictions.argmax(axis=-1)        \n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApjsKL-ckSAi",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbZLwxCjkKb_",
        "colab_type": "text"
      },
      "source": [
        "3.1 Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0f7tYslj4IV",
        "colab_type": "code",
        "outputId": "a115b3b8-f5d0-4c60-a799-d1120d28eee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"NB, Count Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"NB, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, Count Vectors:  0.8368\n",
            "NB, WordLevel TF-IDF:  0.84\n",
            "NB, N-Gram Vectors:  0.8388\n",
            "NB, CharLevel Vectors:  0.8216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qQv2bSg84wL",
        "colab_type": "text"
      },
      "source": [
        "3.2 Linear Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs-mKSaX89ab",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic/sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxbsCc9B8z69",
        "colab_type": "code",
        "outputId": "fe69e671-8a3d-497f-b212-98c30fb3ecd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"LR, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR, Count Vectors:  0.856\n",
            "LR, WordLevel TF-IDF:  0.8656\n",
            "LR, N-Gram Vectors:  0.8384\n",
            "LR, CharLevel Vectors:  0.842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIld1Q3a9h_B",
        "colab_type": "text"
      },
      "source": [
        "3.3 Implementing a SVM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wge0-Zvd9leZ",
        "colab_type": "text"
      },
      "source": [
        "Support Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. The model extracts a best possible hyper-plane / line that segregates the two classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEDs6oR79Lpv",
        "colab_type": "code",
        "outputId": "6d557dcb-d5c3-4132-b784-d0549ce4d42f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors: \", accuracy)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM, N-Gram Vectors:  0.8408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNzCMbwL916o",
        "colab_type": "text"
      },
      "source": [
        "3.4 Bagging Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tixigdd196qB",
        "colab_type": "text"
      },
      "source": [
        "Random Forest models are a type of ensemble models, particularly bagging models. They are part of the tree based model family."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzr2GJU39rNU",
        "colab_type": "code",
        "outputId": "c5e83612-e398-43c1-e1c8-95707376bcb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"RF, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF, Count Vectors:  0.8272\n",
            "RF, WordLevel TF-IDF:  0.8324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r8P8Oby-F33",
        "colab_type": "text"
      },
      "source": [
        "3.5 Boosting Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stzDifvJ-N9L",
        "colab_type": "text"
      },
      "source": [
        "Boosting models are another type of ensemble models part of tree based models. Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA1_TbNE-Bl_",
        "colab_type": "code",
        "outputId": "f6cf1b98-d460-4132-88f2-c9cd09fc506f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Extreme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print(\"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print(\"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xgb, Count Vectors:  0.792\n",
            "Xgb, WordLevel TF-IDF:  0.7936\n",
            "Xgb, CharLevel Vectors:  0.8016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf8CBi95-gDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}